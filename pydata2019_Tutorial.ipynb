{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pydata2019_Tutorial.ipynb","provenance":[{"file_id":"1IA1CK-TaL7Iay1237Qer4X5HwQeifKhT","timestamp":1571939295165},{"file_id":"144tR4R2xv7vXtQ1Q_BpOCK-LphrHmtWT","timestamp":1552778571820},{"file_id":"1nN4Ib_sbXXq1HeIJq8gc3J2zRi8we4YR","timestamp":1552488728668}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CK3qDmsgZbl8","colab_type":"text"},"source":["# PyData 2019 Deep Learning Workshop Tutorial- Pipple & 510\n","\n","This notebook runs you through all basic functionalities that Keras has to offer when building and training a Convolutional Neural Network (CNN). The notebook is configured to be used in Google Colaboratory: a free of use Jupyter Notebook environment running on Google servers.\n","\n","Notebooks are documents which contain both computer code (e.g. python) and rich text elements (paragraph, equations, figures, links, etc…). Notebooks are both human-readable documents containing the analysis description and the results (figures, tables, etc..) as well as executable commands which can be run to perform data analysis. Google Colaboratory combines these features with other Google services such as Google Drive. \n","\n","Before continuing this tutorial make sure you select 'GPU' as hardware accelerator. This will speed up the time of training a CNN substantially. \n","\n","Runtime -> Change runtime type -> Hardware accelerator = 'GPU' -> Save\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P6DQsbvz4sqD","colab_type":"text"},"source":["# Table of Contents\n","\n","\n","1.   Problem Description\n","2.   Retrieve data\n","3.   Convolutional Neural Networks (CNN)\n","4.   Image Data Generators\n","5.   Transfer Learning + Fine-Tuning a CNN (in Keras)\n","6.   Training a CNN (in Keras)\n","7.   Analysing Training Results\n","8.   Play Around!\n","9.   Challenge!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EjTOY5No8jvB","colab_type":"text"},"source":["---\n","---\n","# 1. Problem Description\n","\n","\n","Every year, approximately 90 thousand people are killed and almost 160 million people are affected by natural disasters, such as earthquakes floods, wildfires, and droughts<sup>[1](#myfootnote1)</sup>. Early September 2017,  hurricane Irma struck the island of Sint Maarten, affecting 90% of the buildings and leaving 7000 people (roughly 17% of the total population) without a house. The effectiveness, efficiency, and swiftness of humanitarian aid in such situations depends on the availability of local information, such as maps. However, in practice this information is often not available. \n","\n","Initiatives like the MissingMaps<sup>[2](#myfootnote1)</sup> project help humanitarian aid organisations to obtain the necessary maps. Through MissingMaps, volunteers can remotely trace buildings, roads, and other\n","information in aerial imagery into OpenStreetMap<sup>[3](#myfootnote1)</sup>, an open source map data base. Subsequently, local volunteers can add more detailed information about the traced objects, e.g. street names and building characteristics. Currently, MissingMaps is the go-to solution for humanitarian aid organisations when up-to-date and complete map data is unavailable. However, the process of acquiring these maps is time consuming, labour intensive, and the quality depends heavily on the skills of the volunteers. \n","\n","As a result 510, the Netherlands Red Cross (NLRC) data team, is investigating the possibility of (partly) automating the mapping process of remote areas using aerial imagery and machine learning. The desired models should be able to detect (the outlines) of buildings within the aerial imagery, and determine additional building characteristics of each building. Both objectives have their own inherent challenges and data needs. Therefore, the project is split  into two: one project will focus on the automatic detection of buildings, and another on automatic classification of building characteristics in remotely sensed imagery. This tutorial will focus on the latter topic by classifying roof type characteristics of buildings, that could indicate the vulnerability of buildings to natural disasters. To be more specific, this tutorial will classify roof shapes, either flat or hipped, of individual buildings in Sint Maarten by the use of pre-trained Convolutional Neural Networks (CNNs), Python, Keras and Tensorflow. Data sets are obtained from aerial imagery and information from OpenStreetMap.\n","\n","\n","---\n","> <sup>[1](#myfootnote1)</sup> https://www.who.int/environmental_health_emergencies/natural_events/en/\n","\n","> <sup>[2](#myfootnote1)</sup> https://www.missingmaps.org\n","\n","> <sup>[3](#myfootnote1)</sup> https://www.openstreetmap.org"]},{"cell_type":"markdown","metadata":{"id":"nbwBbO7LGOIT","colab_type":"text"},"source":["\n","\n","---\n","---\n","\n","# 2. Retrieve Data\n","\n","Retrieving images of roof shapes of individual building in Sint-Maarten can be done by running the cell below. Note that the images are divided in a flat and hipped labeled folder which are part of either the train or validation data set. It is important to properly structure the data before you start tackling an image recognition task. In this tutorial the problem will be tackled using a the high-level API Keras. Below figure illustrates a simplified structure of the data.\n","\n","\n","![alt text](https://cdn-images-1.medium.com/max/800/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg)\n","\n"]},{"cell_type":"code","metadata":{"id":"1bkYxvqyMZZT","colab_type":"code","colab":{}},"source":["!git clone https://github.com/PippleNL/pydata2019.git\n","\n","\n","import zipfile\n","\n","for sets in ['train', 'validation']:\n","\n","  # path to zip\n","  local_zip = f'pydata2019/{sets}.zip' \n","\n","  # extract zip file\n","  zip_ref = zipfile.ZipFile(local_zip, 'r')\n","  zip_ref.extractall('/tmp')\n","  zip_ref.close()\n","\n","import os\n","\n","base_dir = '/tmp/'\n","\n","# data directories\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjOq4lTWQd73","colab_type":"text"},"source":["The data sets are located in the /tmp/train and /tmp/validation folders, respectively. Some basic statistics are given below:\n","\n","* The data is seperated into a training and validation set.\n","* The images belong to 2 classes; flat and hipped.\n","* The training set contains 3786 flat and 1467 hipped images.\n","* The validation set contains 811 flat images and 733 hipped images.\n","\n","Below are some bash commands listed to guide you through the repository and let you get a feeling of how the data set is structured."]},{"cell_type":"code","metadata":{"id":"6__VAtjQM_0Z","colab_type":"code","colab":{}},"source":["!ls '/tmp'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vE01TvcgQz96","colab_type":"code","colab":{}},"source":["!ls '/tmp/train'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1sNtveIQ6HE","colab_type":"code","colab":{}},"source":["!ls '/tmp/validation'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAxZQsMH9aQF","colab_type":"code","colab":{}},"source":["!find '/tmp/validation/hipped' -type f -name \"*.tif\" | wc -l"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UNhLXgXOFRM","colab_type":"text"},"source":["---\n","---\n","## 3. Convolutional Neural Networks (CNNs)\n","\n","Traditionally, image classiﬁcation consisted of three steps: (1) local image feature extraction, (2) feature coding, and (3) classiﬁcation. The ﬁrst step involves the use of predeﬁned feature extraction algorithms (e.g. SIFT) which extract local features from several patches within the image. Subsequently, the local image features are encoded by coding algorithms to generate more sparse features suitable for classiﬁcation. At last, the encoded image features are fed into a classiﬁer. \n","\n","In the past years image classiﬁcation problems have been dominated by CNN architectures, which signiﬁcantly outperform all previous state-of-the-art benchmarks<sup>[4](#myfootnote1)</sup>. CNN architectures are typically built out of convolutional layers, pooling layers, and fully connected layers. In these architectures, the convolutional layers and pooling layers are responsible for image feature extraction, and the fully connected layers for classiﬁcation.\n","\n","\n","![alt text](https://miro.medium.com/max/1569/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n","\n","The outstanding performance of CNNs in remote sensing is mainly due to the use of transfer learning: complex deep CNN architectures are pre-trained on enormous image databases (e.g. ImageNet<sup>[5](#myfootnote1)</sup>) and used in other image classiﬁcation problems. The idea is that pre-trained CNNs can generate useful features for almost any classification problem. Classifying these features, however, is different in all cases dependent on the classification problem and the target data set. Generally, there are three approaches when using existing CNN architectures on another data set: \n","1. training the CNN from scratch using the target data set, \n","2. ﬁne-tuning the CNN on the target data set, and \n","3. using the CNN directly as feature extractor in combination with another classiﬁer. \n","\n","What approach achieves the best results depends on the characteristics of the target data set, and the data set the CNN was initially trained on. More information on CNNs and transfer learning for this specific use-case of classifying building characteristics can be found in Bart van Driel's Masters Thesis: Roof Type Classification in Aerial Imagery Using Convolution Neural Networks<sup>[6](#myfootnote1)</sup>.\n","\n","In this tutorial we will guide you in how to fine-tune an existing CNN on the target data set by the use of TensorFlow and Keras (i.e. high-level API built on top of TensorFlow). \n","\n","---\n","\n","><sup>[4](#myfootnote1)</sup> Krizhevsky et al., Imagenet classification with deep convolutional neural networks, 2012\n","\n","><sup>[5](#myfootnote1)</sup> Deng et al., Imagenet: A large-scale hierarchical image database\n","\n","><sup>[6](#myfootnote1)</sup> https://pure.tue.nl/ws/portalfiles/portal/125083941/Master_Thesis_Bart_van_Driel.pdf"]},{"cell_type":"markdown","metadata":{"id":"NSzyTkwfR8dh","colab_type":"text"},"source":["\n","\n","---\n","---\n","\n","## 4. Image Data Generators\n","\n","When fine-tuning a CNN on the target data set, a CNN needs to be provided data (i.e. images). Loading large amounts of images into memory, however, quickly raises memory issues for most servers. Keras deals with this by introducing the so-called ImageDataGenerator objects.\n","\n","ImageDataGenerator objects generate batches of tensor image data with real-time data augmentation. Data augmentation is used to enlarge the amount of train data and is proven to be an effective method to increase a model's performance. \n","\n","![alt text](https://nanonets.com/blog/content/images/2018/11/1_C8hNiOqur4OJyEZmC7OnzQ.png)\n","\n","By doing this real-time only batches of data are loaded into memory such that training the model is much more efficient.\n","\n","Next to loading images into memory, the ImageDataGenerator objects deal with reshaping the images to the desired input size. As you might imagine, it is important that all images of the train data set are reshaped to the size of the images on which the pre-trained CNN is trained. If not, the pre-trained CNN will not be able to extract the right deep features out of the images, which is necessary to classify the train set.\n","\n","In the cells below one can see how the ImageDataGenerators are called and where these generator parameters can be adjusted to a customer's needs. Note that each pre-trained CNN has its own pre-processor function and target size.\n","\n","For more information on Keras' ImageDataGenerators please visit: https://keras.io/preprocessing/image/#imagedatagenerator-class"]},{"cell_type":"code","metadata":{"id":"0PtnTTcK-M_P","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n","from keras.applications.inception_v3 import preprocess_input as preprocess_input_inception\n","from keras.applications.xception import preprocess_input as preprocess_input_xception\n","\n","seed = 42  # Make the Image Data Generator objects reproducible\n","\n","def get_cnn_data_generators(cnn):\n","  \"\"\"\n","  Function that returns Image Data Generator objects for the train and validation set related to a pre-trained cnn (i.e. VGG16, Inception, Xception)\n","  \"\"\"\n","  # Different pre-trained CNN's have different images pre-processor functions\n","  if cnn == 'vgg16':\n","    pre_processor = preprocess_input_vgg16\n","  elif cnn == 'inception':\n","    pre_processor = preprocess_input_inception\n","  elif cnn == 'xception':\n","    pre_processor = preprocess_input_xception\n","  else:\n","    raise ValueError(f'Unknown pre-trained CNN. Got {cnn} whereas vgg16, inception or exception is expected.')\n","\n","\n","  # Below Image Data Generator object is related to the train data set\n","  # This generator is typically allowed to augment images such that the train data set can be enlarged\n","  train_datagen = ImageDataGenerator(\n","    preprocessing_function=pre_processor,  # Most of the pre-trained CNN's (i.e. transfer learning) come with a specific pre-processor function (i.e. required)\n","    rotation_range=180,  # The Image Data Generator object is allowed to rotate images up to 180 degrees (i.e. augmentation)\n","    horizontal_flip=True,  # The Image Data Generator is allowed to horizontally flip images (i.e. augmentation)\n","    vertical_flip=True,  # The Image Data Generator is allowed to vertically flip images (i.e. augmentation)\n","    # Type here your (extra) Image Data Generator specifications related to the train data set (i.e. augmentation)\n","    # For more information visit the link above\n","  )\n","\n","\n","  # Below Image Data Generator object is related to the validation set\n","  # This generator is typically NOT allowed to augment images to accurately measure your CNN's performance on real data!\n","  val_datagen = ImageDataGenerator(preprocessing_function=pre_processor)\n","\n","  return train_datagen, val_datagen"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnUzXy6ZQufT","colab_type":"text"},"source":["To generate batches of real-time (augmented) data, Keras' ImageDataGenerators come with the so called flow_from_directory() method. This method directly links the images directories on the server's local file system to facilitate real-time batching. In this method one also specifies the target reshape size of the train and validation images."]},{"cell_type":"code","metadata":{"id":"fK1HtXX4QnmF","colab_type":"code","colab":{}},"source":["def get_image_batches(train_datagen, val_datagen, cnn):\n","  \"\"\"\n","  Takes the path to a directory & generates batches of (augmented) data reshaped to the desired input shape\n","  \"\"\"\n","  # Different pre-trained CNN's use different target images sizes as input \n","  if cnn == 'vgg16':\n","    target_size = (224, 224)  # All images will be resized to 224x224\n","  elif cnn == 'inception':\n","    target_size = (299, 299)  # All images will be resized to 299x99\n","  elif cnn == 'xception':\n","    target_size = (299, 299)  # All images will be resized to 299x299\n","  else:\n","    raise ValueError(f'Unknown pre-trained CNN. Got {cnn} whereas vgg16, inception or exception is expected.') \n","\n","\n","  # Keras is able to directly augment and use images out of folders from the server's local file system using the flow_from_directory method\n","  # Below example uses the train Image Data Generator to pre-process and augment the images from the train_dir (on the server's local file system)\n","  # and resizes them to the CNN specific target input (e.g. the input resolution for the pre-trained VGG16 CNN)\n","  train_generator = train_datagen.flow_from_directory(\n","    train_dir,  # The train data set directory on the local file system\n","    target_size=target_size,  # All images will be resized to the CNN specific target input\n","    batch_size=16,  # Keras will send batches of 16 (augmented) train images to the CNN\n","    class_mode='sparse',  # Label array will be 1D integer labels \n","    shuffle=True,  # Shuffling is typically used when generating a train data set \n","    seed=seed)  # The seed used for reproducibility for shuffling and augmentation transformations\n","\n","\n","  # Below examples uses the validation Image Data Generator to pre-process the images from the validation_dir (on the server's local file system)\n","  # In general you want the same parameters settings as specified for the train data set\n","  validation_generator = val_datagen.flow_from_directory(\n","    validation_dir,  \n","    target_size=target_size, \n","    batch_size=16,  \n","    class_mode='sparse',\n","    shuffle=False)  # However, shuffling is typically not desired for interpreting the results on the validation data set\n","\n","  return train_generator, validation_generator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uk0vQjA0HG0E","colab_type":"text"},"source":["One way to call a batch of (augmented) images using ImageDataGenerator objects  is by method next(). By running the cells below one will get an idea of how such a train batch looks like, defined by the ImageDataGenerator's parameters."]},{"cell_type":"code","metadata":{"id":"icqfQySdH6hc","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib inline\n","\n","\n","# retrieve Image Data Generator objects for the train set related to the Xception pre-trained CNN\n","pre_trained_cnn = 'xception'\n","train_data_gen, _ = get_cnn_data_generators(cnn=pre_trained_cnn)\n","\n","# Generate batches of augmented train data using the flow_from_directory function in Keras\n","train_batch_gen, _ = get_image_batches(train_data_gen, _, cnn=pre_trained_cnn)\n","\n","# return a batch of (augmented) train images and their labels\n","train_images, train_labels = train_batch_gen.next()\n","\n","# plot the batch of (augmented) images\n","plt.figure(figsize=(25, 10))\n","for i, image in enumerate(train_images, 1):\n","  plt.subplot(4, 5, i)  \n","  plt.grid(False)\n","  plt.title(list(train_batch_gen.class_indices)[int(train_labels[i-1])])\n","  plt.axis('off')\n","  plt.imshow(image)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJS_q0G5XHzm","colab_type":"text"},"source":["---\n","---\n","## 5.   Transfer Learning + Fine-Tuning a CNN (in Keras)\n","\n","Transfer Learning basically includes loading a pre-trained CNN's parameters and its architecture (i.e. structure of layers). In this tutorial one gets the ablilty to choose between 3 different pre-trained CNNs:\n","\n","\n","*   VGG-16: Nework consisting of 23 layers in 5 blocks, where 2 or 3 convolutional layers are followed by a pooling layer. Has 138.357.544 trainable parameters and a size of 528 MB.\n","*   InceptionV3: Network consiting of 314 layers and is made up of 11 so-called inception blocks, making the network extremely deep but manageable in the number of trainable parameters. Has 23.851.784 trainable parameters and a size of 92 MB.\n","*   Xception: Network consiting of 134 layers and uses blocks similar to InceptionV3's inception blocks. Has 22.910.480 trainable parameters and a size of 88 MB.\n","\n","\n","For more information on these pre-trained CNNs, I would like to redirect you to Bart van Driel's Masters Thesis<sup>[6](#myfootnote1)</sup>. \n","\n","\n","Fine-tuning a pre-trained CNN makes it usable for classifying roof shapes. This is done by removing all top classification layers of the pre-trained model and replacing these by newly built (with randomly assigned parameters) classification layers. These newly built classification layers are the ones that will be trained! The cell below defines a function that does this trick and builds a CNN model from an existing pre-trained CNN. By default only the newly added classification layers will be trained. One, however, could choose to re-train some of the pre-trained feature extraction layers to make them more relevant for the classification of roof types. In that case, the parameters will be trained starting from the parameter values that were subject to the train results of the imagenet classification problem. Feel free to experiment with any kind of newly built classification layers. More information on different classification layers and how to deploy them using keras, can be found in<sup>[7](#myfootnote1)</sup>.\n","\n","\n","\n","---\n","\n","\n","><sup>[6](#myfootnote1)</sup> https://pure.tue.nl/ws/portalfiles/portal/125083941/Master_Thesis_Bart_van_Driel.pdf\n","\n","> <sup>[7](#myfootnote1)</sup> https://keras.io/layers/core/"]},{"cell_type":"code","metadata":{"id":"jklItf33a8MK","colab_type":"code","colab":{}},"source":["from keras.applications.vgg16 import VGG16\n","from keras.applications.xception import Xception\n","from keras.applications.inception_v3 import InceptionV3\n","\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D, Input, Flatten\n","from keras.layers import Dropout, BatchNormalization\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")  # surpress library warnings\n","\n","\n","def get_cnn_model(cnn):\n","  \"\"\"\n","  Retrieves the parameters and architecture of pre-trained CNNs (on ImageNet data) without the top (classification) layers.\n","  It builds on top of these retrieve (feature extraction) layers a block of layers used for classification. \n","  \"\"\"\n","  # Retrieve the pre-trained cnn on images of imagenet without the top-classification layers\n","  if cnn == 'vgg16':\n","    base_model  = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","  elif cnn == 'inception':\n","    base_model  = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n","  elif cnn == 'xception':\n","    base_model  = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n","  else:\n","    raise ValueError(f'Unknown pre-trained CNN. Got {cnn} whereas vgg16, inception or exception is expected.') \n","\n","\n","  # Set all retrieved cnn (feature extraction) layers to non-trainable; adjust to own preference\n","  for layer in base_model.layers:\n","      layer.trainable = False\n","\n","\n","  # Add extra (trainable) classification layers on top of the last feature extraction layer that is part of the pre-trained cnn\n","  # Make sure that these classification layers are imported from the keras.layers module\n","  x = base_model.output  # Retrieve last feature extraction layer (i.e. non-trainable)\n","  x = Flatten()(x)  # Add flatten layer  (i.e. does not consist of parameters to be trained) \n","  x = Dense(256, activation='relu', name='fc1')(x)  # Add the first non-pre-trained convolutional (classification) layer (i.e trainable)\n","  x = Dense(256, activation='relu', name='fc2')(x)  # Add the second non-pre-trained convolutional (classification) layer (i.e trainable)\n","  \n","  # Add the last non-pre-trained convolutional (classification) layer (i.e trainable) generating prediction output.\n","  preds = Dense(2, activation='softmax', name='preds')(x) # Make sure this layer has the same number of nodes as the number of classes in the classification problem\n","\n","\n","  # Build the model having non-trainable pre-trained feature extration layers and trainable classification layers\n","  model = Model(inputs=base_model.input, outputs=preds)\n","\n","  return model "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3yf1yQQNr7X","colab_type":"text"},"source":["---\n","---\n","## 6.   Training a CNN (in Keras)\n","\n","Training a CNN that consists of pre-trained layers and newly built classification layers can be done by calling the Keras' fit_generator method <sup>[8](#myfootnote1)</sup>. This method trains the model on data generated batch-by-batch by an ImageDataGenereator object which facilitates real-time data augmentation. \n","\n","The fit_generator method is applied to a so-called compiled model, which basically states how Keras should train the model. By compiling the model, one configures the optimizer<sup>[9](#myfootnote1)</sup>, loss function<sup>[10](#myfootnote1)</sup> and evaluation metrics<sup>[11](#myfootnote1)</sup> that are used while training. \n","\n","The training results, that can be used to analyse the training progress or to compare models, are logged by so-called Keras callbacks<sup>[12](#myfootnote1)</sup>. In these callbacks one defines all metrics that need to be stored (and in which files) to make a fair comparison between training results of different models. \n","\n","\n","\n","---\n","\n","><sup>[8](#myfootnote1)</sup> https://keras.io/models/sequential/\n","\n","><sup>[9](#myfootnote1)</sup> https://keras.io/losses/\n","\n","><sup>[10](#myfootnote1)</sup> https://keras.io/optimizers/\n","\n","><sup>[11](#myfootnote1)</sup> https://keras.io/metrics/\n","\n","><sup>[12](#myfootnote1)</sup> https://keras.io/callbacks/\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RGrB9XLbJQeX","colab_type":"text"},"source":["The cell below defines a function in which a CNN model is compiled. Note that compiling can only be done after building a CNNs architecture!\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"WnuvJefAP-GP","colab_type":"code","colab":{}},"source":["from keras.optimizers import SGD\n","\n","\n","def build_compile_cnn(cnn):\n","  \"\"\"\n","  Builds a CNN architecture based on one of the three pre-trained CNNs (on ImageNet data) and extra added classification layers.\n","  Compiles this built cnn by specifying a loss function, optimizer and evaluation metric.\n","  \"\"\"\n","\n","  # Retrieve feature extraction layers of the pre-trained CNN and add newly to be trained (classification) layers\n","  cnn_model = get_cnn_model(cnn)\n","\n","\n","  # Compile the model\n","  cnn_model.compile(loss='sparse_categorical_crossentropy',  # Depends on how the target labels (e.g. labelled images in the data sets) are defined; are integers in this tutorial\n","                    optimizer=SGD(),  # Stochastic gradient descent optimizer used to optimize parameters; note that when choosing a different optimizer it needs to be imported from the keras.optimizers library\n","                    metrics=['acc'])  # Typically one uses the model's accuracy\n","  \n","  return cnn_model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BA5_7BfLOhyO","colab_type":"text"},"source":["The cell below defines a function that initialises some Keras callbacks. To make it more understandable, one could say that there are 2 ways of storing training information (i.e. defining callbacks):\n","\n","*   In seperate files: A callback writing training info in a (text) file. Examples are the loss function metric in each training iteration and the model's parameter values for the interation which achieved the best results (i.e. CSVLogger and ModelCheckPoint)\n","*   In a TensorBoard event folder: A callback that stores evaluation metrics that can interactively be accessed using TensorBoard software. TensorBoard will be used in this tutorial to analyse and compare training results of different models. By default a TensorBoard callback only saves metrics as accuracy and the loss for the train and validation set. If one wants to add metrics to this callback one could do this by following the code in the cell below. \n","\n","\n","Moreover, make sure that each training run is stored in a unique file name or event folder. Otherwise, there is nothing to compare against.\n","\n"]},{"cell_type":"code","metadata":{"id":"yh76SOXOJiVL","colab_type":"code","colab":{}},"source":["from keras.callbacks import CSVLogger, ModelCheckpoint, TensorBoard\n","from os.path import join, exists\n","from keras import backend as K\n","from keras.callbacks import TensorBoard\n","import time\n","from os import makedirs\n","\n","\n","# Adding the learning rate of the training procedure to the TensorBoard event folder \n","class LRTensorBoard(TensorBoard):\n","    def __init__(self, log_dir, **kwargs): \n","        super().__init__(log_dir=log_dir, **kwargs)\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs.update({'lr': K.eval(self.model.optimizer.lr)})  # Store learning rate as lr in the event folder\n","        super().on_epoch_end(epoch, logs)\n","\n","\n","def get_callbacks(model_name):\n","  \"\"\"\n","  Instantiates different Keras Callbacks used to analyse and compare training results\n","  \"\"\"\n","  # Define unique training ID such that logging events are stored in a unique folder\n","  model_id = time.strftime('%Y-%m-%d_%H-%M-%S')\n","\n","  # Callback that streams epoch results to a csv file.\n","  callback_csv = CSVLogger(filename=join(base_dir, f'{model_name}_{model_id}.csv'), \n","                          separator=',', \n","                          append=False)  \n","  # Callback that saves the model after every epoch\n","  os.makedirs(join(base_dir, 'models'), exist_ok=True)  # create models directory if not already present\n","  callback_model = ModelCheckpoint(filepath=join(base_dir, 'models', f'{model_name}_{model_id}.hdf5'), \n","                                  monitor='val_acc', verbose=1, \n","                                  save_best_only=True, save_weights_only=True,  # both are set to TRUE to increase the training speed\n","                                  mode='auto', period=3)  \n","  # Callback that writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics\n","  # as well as activation histograms for the different layers in your model. \n","  # (https://www.tensorflow.org/tensorboard/)\n","  callback_tensorboard = TensorBoard(log_dir=join(base_dir, 'logs', f'{model_name}_{model_id}'), \n","                                     histogram_freq=0,\n","                                     write_graph=True,\n","                                     write_grads=True,\n","                                     batch_size=train_batch_gen.samples/train_batch_gen.batch_size,\n","                                     write_images=True)\n","   \n","  # Add Learning rate to tensorboard logging events\n","  callback_learning_rate = LRTensorBoard(log_dir=join(base_dir, 'logs', f'{model_name}_{model_id}'))\n","\n","  return [callback_csv, callback_model, callback_tensorboard, callback_learning_rate]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSEsgvkcn14P","colab_type":"text"},"source":["The cell below calls the fit_generator method for the compiled CNN model. Note that in this cell all other previous defined functions are also called to create the necessary CNN architecture, configuration, data generators and callback objects. "]},{"cell_type":"code","metadata":{"id":"gB9iCdykp1TG","colab_type":"code","colab":{}},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","# Define the pre-trained CNN that will be used for transfer learning and classification of roof types\n","pre_trained_cnn = 'xception'  # 'vgg16', 'inception' or 'xception'\n","\n","\n","# Build and compile a to be trained cnn model (i.e. includes the model architecture, initial parameters, optimizer, loss and evaluation function)\n","cnn_model = build_compile_cnn(cnn=pre_trained_cnn)\n","\n","\n","# Create corresponding train and validation data generators that specify augmentation rules\n","train_data_gen, vali_data_gen = get_cnn_data_generators(cnn=pre_trained_cnn)\n","\n","\n","# Create generators to generate batches of augmented train data using the flow_from_directory function in Keras\n","train_batch_gen, vali_batch_gen = get_image_batches(train_data_gen, \n","                                                    vali_data_gen, \n","                                                    cnn=pre_trained_cnn)\n","\n","\n","# It might be interesting to add class weights as a parameter to the fit_generator method such that training can 'pay more attention' to samples from an under-represented class\n","# Uncomment below lines to calculate the class weights\n","# class_weights = compute_class_weight('balanced', \n","#                                      np.unique(train_batch_gen.classes), \n","#                                      train_batch_gen.classes)\n","\n","# Create callbacks that log training results\n","callbacks = get_callbacks(model_name=pre_trained_cnn)\n","\n","\n","# Fit the CNN to training and validation data; add class_weights as a parameter when one wants to 'pay more attention' to samples from an under-represented class\n","trained_cnn = cnn_model.fit_generator(generator=train_batch_gen,  # ImageDataGenerator for the training data set\n","                                      steps_per_epoch=15,  # Total number of steps to yield from generator use steps_per_epoch=train_batch_gen.samples/train_batch_gen.batch_size to use a full trainingset per epoch\n","                                      epochs=3,  # Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by steps_per_epoch\n","                                      verbose=1,  # 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n","                                      callbacks=callbacks,  # List of callbacks to apply during training\n","                                      validation_data=vali_batch_gen,  # ImageDataGenerator for the validation data set\n","                                      validation_steps=vali_batch_gen.samples/vali_batch_gen.batch_size,  # Total number of steps to yield from validation_data\n","                                      )\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ok6la34PxK--","colab_type":"text"},"source":["---\n","---\n","\n","## 7.   Analysing Training Results\n","\n","Analysing training results is done using TensorBoard in this tutorial. TensorBoard is TensorFlow's visualisation toolkit to track and visualise performance metrics that are stored via the earlier defined callbacks. \n","\n","Tensorboard visualises different training event logs. Consequently, when training multiple models, make sure that all events are stored in a unique folder. One typically uses TensorBoard's visualisation to determine the epoch number after which training results become stale or how different learning rates influence the level of (optimization) convergence.\n","\n","By running the cell below one starts an interactive TensorBoard session where training results can be analysed. TensorBoard also facilitates different tools such as displaying image data (even their features in specific layers)! More information on TensorBoard can be found in <sup>[13](#myfootnote1)</sup>\n","\n","\n","---\n","\n","><sup>[13](#myfootnote1)</sup> https://www.tensorflow.org/tensorboard/image_summaries\n","\n"]},{"cell_type":"code","metadata":{"id":"m0UdYuHyj7gT","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir {join(base_dir, 'logs')}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qV4pfMJT1rHI","colab_type":"text"},"source":["---\n","---\n","\n","## 8.   Play Around!\n","\n","\n","Congratulations! You finished the tutorial and fine-tuned a pre-trained Xception CNN to classify roof types of buildings in Sint Maarten! Of course there is still a lot more to discover about image recognition so try to play around a bit and use TensorBoard to see what the impact is on the training results.\n","\n","Some suggestions to adjust in the tutorial:\n","\n","*   Try 'vgg16' or 'inception' as the pre-trained CNN used in transfer learning\n","*   Try adjusting the node size of the newly built classification layers in your CNN\n","*   Try adding an extra classification layer when building the model\n","*   Try training the model for more epochs\n","*   Try training the model for more steps_per_epoch\n","*   Try training the model with a different optimizer\n","*   Try adjusting the data augmentation rules of the ImageDataGenerators\n","*   Try adding class weights to the fit_generator method to 'pay more attention' to samples from an under-represented class\n","\n","Enjoy!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7WwCR2T_YCZr","colab_type":"text"},"source":["\n","\n","---\n","---\n","\n","## 9.   Challenge!\n","\n","\n","You're now ready to start the challenge and create your own CNN algorithm to classify the roof materials of buildings in Sint Maarten! Use this tutorial as a starting point and try to be creative. Pipple and 510, The Netherlands Red Cross data team, are very curious of the results.\n","\n","The challange notebook can be opened in the same way as this tutorial is opened, except for the fact that you should now choose \"Pydata2019_Challenge_Materials.ipynb\" in \"Path\". For more information you can have a look at today's sent email!"]}]}